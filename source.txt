
#1 Introduction

- ease of development
- some WGs have dropped the "running code" principle (see "DNS Camel" talk)
- composability = reuse
- minimize attack surface
- rapid prototyping

#1 Terminology

The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD", "SHOULD NOT", "RECOMMENDED",  "MAY", and "OPTIONAL" in this document are to be interpreted as described in RFC 2119.

#1 Principles

- no udefined state (fail-fast)
  - socket options: what happens if set in the middle of socket's lifetime?
  - errors: what state is the socket left in when function X returns error Y?
  - undefined state is a huge attack surface
  - seems to violate Postel's principle, but not really -- the implementation can still be liberal, but it must be consciously liberal (i.e. implementer is explicit about stuff that is allowed)
- kernel/user space agnostic 
  - deploying in kernel space can take years
  - unikernels and such
  - artificial split between core protocols and application protocols
  - also, it's slower
  - also, being in the user space it has less privileges
- the other way round: sanitize the API to steer protocol implementers towards sane protocol design
  - good practices should be easy to implement
  - bad practices should be hard to implement
- encapsulation
  - without composability, all layers will end up intertwined
  - it's easier to reason about nicely encapsulated microprotocol
- two kinds of users
  - protocol above
  - application or "orchestrator"
- parametrizable protocols
  - example of size-prefixed messages
  - show how this can be used to avoid extra initial roundtrips
- different kinds of protocols
  - application vs. transport
    - application has create/destroy APIs, but no send/recv APIs
    - transport has both
    - transport can be either bytestream or message-oriented
  - presentation layer is not addressed, up to the user to decide whether to model it as application or transport
  - protocols with init/term vs. without it

#2 Vertical composability

Vertical composability is an ability to stack protocols one on the top of another. The protocol on the top exists within the payload of the protocol beneath it.

Example of vertical stack of protocols:

%
+----------+
|   HTTP   |
+----------+
|    TCP   |
+----------+
|    IP    |
+----------+
| Ethernet |
+----------+
%

#2  Horizontal composability

Horizontal composability is an ability to execute protocols in sequential manner. When two protocols are composed in this manner one type of communication ends to be replaced by another type of communication. In other words, one protocol is terminated and another one is started on the top of the same underlying protocol.

An example of horizontal composability is how WebSocket begins with HTTP handshake which is, in turn, followed by core WebSocket protocol:

%
+----------------------------------+
|   HTTP   |       WebSocket       |
+----------+-----------------------+
|                TCP               |
+----------------------------------+
%

Note how this pattern makes protocols reusable: The same HTTP handshake can be used for both trasferring simple HTML web page and for initiating a WebSocket session. Also, either part could be used in the future as a component of new, yet unforeseen protocols.

While this is a very visible case of horizontal composition of protocols the technique is in fact ubiquitous. For example, most protocols are composed from three distinct mini-protocols: protocol header (initial handshake), protocol body (sending data back and forth) and protocol footer (terminal handshake):

%
+-----------------------------------+
| Header |       Body      | Footer |
+--------+-----------------+--------+
|                TCP                |
+-----------------------------------+
%

The requirement for horizontal protocol composability has important consequences for the API design.

When designing a network protocol, it's common to ignore the protocol termination semantics and just default to something like "just close the underlying TCP connection". However, if we want to be able to start a new protocol on top of the underlying protocol, that's no longer an option. We have to terminate the old protocol without destroying the entire protocol stack. Moreover, we want the termination to proceed in orderly manner, so that after it is done, both peers can agree where exactly, in the stream of bytes they are. If they can't, there's no way to start the new protocol.

Thus, further on in this document, there will be a lot of discussion about orderly termination of protocols.

#2 The nature of protocol

When thinking of network protocols we tend to think of some kind of self-contained entity. ...

#2 Scheduling

During the decades since BSD sockets were first introduced the way they are used have changed significantly. While in the beginning the user was supposed to fork a new process for each connection and do all the work using simple blocking calls nowadays they are expected to keep a pool of connections, check them via functions like "poll" or "kqueue" and dispatch any work to be done to one of the worker threads in a thread pool. In other words, user is supposed to do scheduling by hand.

This changes happened for performance reasons and haven't improved functionality or usability of BSD socket API in any way. On the contrary, by requiring every programmer to do system programmer's work it contributed to proliferation of buggy, hard-to-debug and barely maintainable network code.

To address this problem, this memo assumes that there already exists an efficient concurrency implementation. It assumes that forking a new lightweight process takes at most hundreds of nanoseconds and context switch takes at worst tens of nanoseconds. Note that there are already such concurrency systems deployed in the wild. One well-known example are Golang's goroutines but there are others available as well.

In the environment with cheap concurrency the network programming can be done in the old "one process per connection" way, with all the functions exhibiting blocking behavior. There's no need for polling, thread pools, callbacks, explicit state machines or similar antics.

This memo thus adheres to "let system programmers do system programming" maxim and doesn't address the problem of scheduling at all.

#1 Handles

Handle is an integer referring to a protocol instance, very much like "socket" or "file descriptor" in POSIX world.

However, given that we want the API to be kernel/user space agnostic, we are going to use a different term.

But despite the difference in terminology, handles are still integers and they can therefore be, if implemented in the kernel space, just a special type of file descriptor.

In user space the situation is somewhat different. POSIX provides no way to create custom sockets or file descriptors. Therefore, user-space implementations of this specification has to make handles a different kind of entity. For example, file descriptor 3 may refer to a file opened using POSIX API. Handle 3 may refer to a TCP connection opened using API defined in this document.

The user MUST NOT assume that POSIX file descriptors and handles live in the same namespace and/or never clash each with the other.

Handles MUST NOT be negative. They MAY be zero. This is in line with the semantics of POSIX file descriptors. Value of -1 is typically used to signal a function failure. 

Handles can be closed using "hclose" function:

%
int hclose(int h);
%

This function is very similar to POSIX "close" function and in fact, in kernel-space implementations of this specification, it may map directly to the POSIX "close" function.

That being said, there's a difference in semantics when compared to BSD sockets: "hclose" MUST never block or leave any processing to be finished in the background. It MUST close the protocol and deallocate all the associated resources immediately. (But note that this behaviour doesn't contradict the POSIX specification of "close" function.)

Note that this means that "hclose" doesn't guarantee orderly shutdown of the protocol. If communication with the peer is impossible, it can even omit sending RST packet (or equivalent). This, in turn, is not a good news for the peer which may be left with a dangling connection. That being the case the user SHOULD use "hclose" function sparingly. It may be appropriate, for example, in a case of a DoS attack, or when the peer violates the protocol specification. To perform an orderly shutdown, individual protocols SHOULD provide dedicated functions.

Handles cannot be duplicated. Even if implemented as file descriptors, POSIX "dup" function MUST return -1 and set errno to ENOTSUP.

On the other hand, handles support a simple mechanism for transferring ownership.

The idea is that the owner is whoever knows the numeric value of the handle. To transfer ownership, the handle is assigned a different number using "hown" function:

%
int hown(int h);
%

For example:

%
int h = tcp_connect(addr, -1);
/* h = 58 */
h = hown(h);
/* h = 12 */
%

Note that, unlike with duplicated file descriptors, transfer of ownership will render all the copies of the original handle invalid.

The semantics of ownership transfer, as described above, are needed to ensure encapsulation of vertically stacked protocols: If protocol B lives on top of protocol A, it can transfer the ownership of A to iself, thus making it impossible for the original owner of A to interfere with B's usage of A.

TODO: Handle refers to a protocol stack, not a protocol. Should this be explained here or elsewhere?

#1 Deadlines

Unlike with BSD sockets the deadlines are points in time rather than intervals. This allows to use the same deadline in multiple calls without need to recompute the timeout interval.

Consider the following pseudocode using the classic interval-based timeouts:

%
int timeout = 1000;
time t1 = now();
send(s, "ABC", 3, timeout);
time t2 = now();
timeout -= (t2 - t1)
if(timeout < 0) return;
send(s, "DEF", 3, timeout);
%

It can be rewritten in a much simpler way using point-in-time-based deadlines:

%
time deadline = now() + 1000;
send(h, "ABC", 3, deadline);
send(h, "DEF", 3, deadline);
%

Function "now" MUST be available.

%
int64_t now(void);
%

It MUST return current time in millisecond precision in a form of 64-bit signed integer. The returned time MUST be a positive number.

The time MUST be monotonic, i.e. it MUST NOT ever move backwards.

The user MUST NOT assume that the epoch (time 1) begins at any particular point in time. It may be an UNIX epoch (Jan 1st, 1970). It may also be the time when the computer was booted. This specification gives no guarantees.

All functions that could possibly block MUST accept a deadline. The deadline is a time, as defined above.

There are two special values that can be passed to any function requiring a deadline: Zero means that the function should not block. If the operation cannot be accomplished immediately, it MUST time out. -1 means no deadline. The function MUST never time out. It can only exit if the operation succeeded or a non-timeout-related error was encountered.

The deadline SHOULD be passed to the function as its last argument. If deadline expires the function should return with ETIMEDOUT error.

Example:

%
int rc = foo(now() + 1000);
if(rc == -1 && errno == ETIMEDOUT) {
    printf("Function foo has not succeeded within 1 second.\n");
    return 1;
}
%

#1 I/O lists

Send and receive functions are using iolist structure with the following definition:

%
struct iolist {
    void *iol_base;
    size_t iol_len;
    struct iolist *iol_next;
    int iol_rsvd;
};
%

It is used for the same purpose that structure iovec is used for in POSIX. However, instead of being assembled in gather/scatter arrays, iolist structures are chained to form singly-linked lists.

* iol_base points to a buffer. For receiving functions it can be NULL, meaning that iol_len number of bytes should be skipped.
* iol_len is the size of the buffer pointed to by iol_base
* iol_next is the next element in the linked list, last element in the list MUST have this fields set to NULL
* iol_rsvd is reserved and MUST be always set to zero by the caller

I/O lists are passed to the API using two arguments: Pointer to the first element of the list and pointer to the last element of the list. For example:

%
int msendl(int s, struct iolist *first, struct iolist *last,
           int64_t deadline);
%

I/O lists are not thread-safe. Functions accepting them as input are allowed to modify them (not the contents of the message though!) but they MUST restore the list into its original state before returning to the caller. The list MUST be restored to its original state even if the function fails.

Function accepting an I/O list as a parameter, unless it just forwards the list to a different function or protocol, MUST check validity of the list. The list is invalid iff:

* last->iol_next is not NULL
* first and last don't belong to the same list
* there's a loop in the list
* iol_rsvd of any item is non-zero
* for sending functions, if iol_base of any element with non-zero length is NULL

TODO: Show some use cases:

- prefix message
- add to message
- replace N bytes

#1 Skipping

TODO: Skipping. The goal of the skipping design is not to require user to allocate memory if they want to skip even large amount od data. This is often needed when implementing terminal handshake: One party sends a termination command, then skips all the incoming data until it gets confirmation of the command.

I/O list element with lol_base set to NULL can be used to skip messages.

To skip arbitrarily large message:

%
sz = mrecv(s, NULL, SIZE_MAX, -1);
%

To read first eight bytes of a message, then skip the rest of it:

%
char buf[8];
struct iolist iol2 = {NULL, SIZE_MAX, NULL, 0};
struct iolist iol1 = {buf, sizeof(buf), &iol2, 0};
sz = mrecvl(s, iol1, iol2, -1);
%

To skip bytes in bytestream protocol, it works in a similar way:

%
rc = brecv(s, NULL, 8, -1);
%

TODO: How to skip until end of bytestream? I don't have a single use case for that. Maybe it's not needed.

#1 Tx buffering

Buffering of outbound data and sending them down the stack in batches often results in improved performance. This specification allows the protocol implementation to do so as long as data is flushed when the particular protocol layer is terminated. However, if done, the data should also be flushed periodically not to induce unbounded latencies when there is not enough new data to fill in the tx buffer. This may, in turn, introduce unwanted complexity to the protocol implementation. It is often preferrable to avoid tx buffering and assume that it will be done somewhere deeper in the stack anyway. 

#1 Rx buffering

Buffering of inbound data collides with vertical composability of protocols.

Consider the following protocol stack:

%
+---+
| C |
+---+
| B |
+---+
| A |
+---+
%

Imagine that B reads 1000 bytes of data from A and puts it into its rx buffer. Further, imagine that C asks B for 500 bytes, leaving remaining 500 bytes in B's rx buffer. Then it asks B to close. What is B going to do with those 500 bytes in its buffer? There's no way to push them back to A. Allowing for such an operation would mean that A's rx buffer would have to be virtually unbounded.

If, on the other hand, the remaining bytes were dropped there would be no way to start a new protocol on top of the same underlyingsocket. The new protocol would miss initial 500 bytes of data.

Luckily though, the above reasoning doesn't apply to the bottommost protocol in the stack. If user closes a bottommost protocol they can't create a new protocol on top of the underlying layer simply because there's no underlying layer. In that case any data remaining in the tx buffer can be dropped.

Also, rx buffering on the lowermost level, where the protocol is interfacing with the hardware or, alternatively, with user/kernel space boundary, is likely to provide the largest performance benefit.  Absence of rx buffering on higher levels, where performance impact of additional receive operation is basically that of a function call, is not likely to incur huge performance penalty. And even more so given that higher layers of the stack are likely to be message-based and thus some amount of batching, proportional to the average message size, happens anyway.

#1 Caveats

- this specification prevents some scenarios
  - specifically, you can't use lower protocol's termination mechanism and still be able to do the "half-close connection, then read the remaining messages" scenario.
- this is probably a good thing
- if needed, because a legacy protocol uses it, it can still be served by this API by wrapping all the protocol layers into a single macro-protocol.
- actually, this technique let's you get away with almost anything
- imagine a specialized send function, e.g. udp_send(s, addr, buf, len)
  - once the socket is wrapped in a different protocol the address can't be specified
  - we have to rely on the default
  - different example of the same: TEXT vs. BINARY messages in WebSocket protocol
    - is that a bad protocol design?
  - in theory sometimes can be solved by having multiple sockets
  - traditional solution: ancillary data
    - some complex that almost noone uses it
- what about STARTTLS?
  - works OK if the layers above can be torn down after STARTTLS
    - this means any state is lost
    - however, SMTP starts the entire negotiation again, so that's not a problem
  - if you can't tear down the layers above, it's a problem
    - does anyone actually do this kind of insane "layer injection" into a living stack?

#1 IANA Considerations

This memo includes no request to IANA.

#1 Security Considerations

- small attack surface
- reusability means that bugs can be fixed in many protocol stacks simultaneously
- Network APIs can facilitate DoS attacks by allowing for unlimited buffer sizes and for infinite deadlines. This proposal avoids the first issue by requiring the user to allocate all the buffers. It addresses the second problem by always making the deadline explicit. Also, by not requiring recomputation of timeout intervals it makes the deadlines easy to use and hard to get wrong. The user should take advantage of that and set reasonable timeout for every network operation.
